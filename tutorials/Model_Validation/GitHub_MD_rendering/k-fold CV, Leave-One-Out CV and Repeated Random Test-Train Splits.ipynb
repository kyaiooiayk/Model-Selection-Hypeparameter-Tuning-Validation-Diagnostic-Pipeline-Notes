{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** k-fold CV, Leave-One-Out CV and Repeated Random Test-Train Splits\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to estimate your ML algorithm accuracy\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-  **k-Fold CV** is the gold standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.\n",
    "- **Train/test split** is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.\n",
    "- Techniques like **leave-one-out** and **repeated random splits** can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.\n",
    "- **The bottom line?** If in doubt, use 10-Fold CV.\n",
    "  \n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import Markdown, display\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:  (768, 9)\n"
     ]
    }
   ],
   "source": [
    "filename = \"../DATASETS/pima-indians-diabetes.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] \n",
    "dataframe = read_csv(filename, names = names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "print(\"Data shapes: \", dataframe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and a test set\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We can take our original dataset and split it into two parts. Train the algorithm on the first part, make predictions on the \n",
    "second part and evaluate the predictions against the expected results.\n",
    "\n",
    "- **CONS**: A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of accuracy.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X size (768, 8)\n",
      "Train X size (514, 8) percentage 66.92708333333334\n",
      "Test X size (254, 8) percentage 33.07291666666667\n",
      "Accuracy on test data:  78.74015748031496\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size, random_state = seed)\n",
    "\n",
    "print(\"Original X size\", X.shape)\n",
    "print(\"Train X size\", X_train.shape, \"percentage\", (X_train.shape[0] / X.shape[0]) * 100 )\n",
    "print(\"Test X size\", X_test.shape, \"percentage\", (X_test.shape[0] / X.shape[0]) * 100 )\n",
    "\n",
    "model = LogisticRegression( max_iter = 1000)\n",
    "# Train your model\n",
    "model.fit(X_train, Y_train)\n",
    "# See how your model is doing on unseen data (_test)\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy on test data: \", result*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold CV\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- CV is an approach that you can use to estimate the performance of a ML algorithm with less variance than a single train-test set split. \n",
    "- It works by splitting the dataset into k-parts. After running cross validation you end up with k different performance scores that you can summarize using a mean and a standard deviation. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold has accuracy:  0.8311688311688312\n",
      "2-fold has accuracy:  0.7402597402597403\n",
      "3-fold has accuracy:  0.7402597402597403\n",
      "4-fold has accuracy:  0.8051948051948052\n",
      "5-fold has accuracy:  0.7922077922077922\n",
      "6-fold has accuracy:  0.7792207792207793\n",
      "7-fold has accuracy:  0.6623376623376623\n",
      "8-fold has accuracy:  0.8051948051948052\n",
      "9-fold has accuracy:  0.8289473684210527\n",
      "10-fold has accuracy:  0.7368421052631579\n",
      "Mean accuracy: 77.2163 with standard deviation of: 4.9684\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=num_folds, shuffle = True, random_state = seed)\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    print(str(i + 1) + \"-fold has accuracy: \", results[i])\n",
    "\n",
    "print(f\"Mean accuracy: {results.mean()*100.0:.4f} with standard deviation of: {results.std()*100.0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leave-One-Out CV\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- You can configure cross validation so that the size of the fold is 1 (k is set to the number of observations in your dataset). \n",
    "\n",
    "- This variation of cross validation is called leave-one-out cross validation. \n",
    "\n",
    "- The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. \n",
    "\n",
    "- CONS: A downside is that it can be a computationally more expensive  procedure than k-fold cross validation.\n",
    "\n",
    "- You can see in the standard deviation that the score has MORE variance than the k-fold cross validation results described above\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 77.6042 with standard deviation of: 41.6894\n"
     ]
    }
   ],
   "source": [
    "loocv = LeaveOneOut()\n",
    "model = LogisticRegression(max_iter = 250)\n",
    "results = cross_val_score(model, X, Y, cv = loocv)\n",
    "print(f\"Mean accuracy: {results.mean()*100.0:.4f} with standard deviation of: {results.std()*100.0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeated Random Test-Train Splits\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Another variation on k-fold cross validation is to create a random split of the data like the train/test split described above, but repeat the process of splitting and evaluation of the algorithm multiple times, like cross validation. \n",
    "\n",
    "- This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation.\n",
    "\n",
    "- **CONS**: A down side is that repetitions may include much of the same data in the train or the test split from run to run, introducing redundancy into the evaluation.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 76.5354 with standard deviation of: 2.2354\n"
     ]
    }
   ],
   "source": [
    "n_splits = 10\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression(max_iter = 500)\n",
    "results = cross_val_score(model, X, Y, cv = kfold)\n",
    "print(f\"Mean accuracy: {results.mean()*100.0:.4f} with standard deviation of: {results.std()*100.0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Methods                           | Type/variation of | Speed                        | Variance                   |\n",
    "| --------------------------------- | ----------------- | ---------------------------- | -------------------------- |\n",
    "| Train/test split                  | NA                | Fastest                      | higher than k-fold         |\n",
    "| k-Fold                            | k-Fold            | Slower than train/test split | less than train/test split |\n",
    "| Leave-one-out                     | k-Fold            | Slower tha k-Fold            | higher than k-Fold         |\n",
    "| Repeated Random Test-Train Splits | Train/split       | Slower than train/test split | less than k-Fold           |\n",
    "\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://machinelearningmastery.com/evaluate-performance-machine-learning-algorithms-python-using-resampling/\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
