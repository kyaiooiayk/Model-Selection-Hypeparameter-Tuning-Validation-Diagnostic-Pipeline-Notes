{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** Nested cross-validation\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20,\n",
    "                           random_state=1, n_informative=10, n_redundant=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-nested vs. nested CVs\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **NON-NESTED** = estimates the generalization error of the underlying model and its (hyper)parameter search.\n",
    "Model selection without nested CV uses the same data to tune model parameters and evaluate model performance. \n",
    "Information may thus “leak” into the model and overfit the data. The magnitude of this effect is primarily \n",
    "dependent on the size of the dataset and the stability of the model. Choosing the parameters that maximize \n",
    "non-nested CV biases the model to the dataset, yielding an overly-optimistic score.\n",
    "\n",
    "\n",
    "- **NESTED** = cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. \n",
    "To avoid this problem, nested CV effectively uses a series of train/validation/test set splits. In the inner loop\n",
    "(here executed by GridSearchCV), the score is approximately maximized by fitting a model to each training set, and\n",
    "then directly maximized in selecting (hyper)parameters over the validation set. In the outer loop (here in \n",
    "cross_val_score), generalization error is estimated by averaging test set scores over several dataset splits\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Manual nested cross-validation for random forest on a classification dataset.\n",
    "\n",
    "- Importantly, we can configure the hyperparameter search to refit a final model with the entire training \n",
    "dataset using the best hyperparameters found during the search. This can be achieved by setting the `refit=True`, then retrieving the model via the 'best_estimator_' attribute on the search result.\n",
    "\n",
    "- We will keep things simple and tune just two hyperparameters with three values each, e.g. (3 * 3) 9 \n",
    "combinations. We will use 10 folds in the outer cross-validation and three folds for the inner cross-validation,\n",
    "resulting in (10 * 9 * 3) or 270 model evaluations. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">acc=0.900, est=0.932, cfg={'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.940, est=0.924, cfg={'max_features': 4, 'n_estimators': 500}\n",
      ">acc=0.930, est=0.929, cfg={'max_features': 4, 'n_estimators': 500}\n",
      ">acc=0.930, est=0.927, cfg={'max_features': 6, 'n_estimators': 100}\n",
      ">acc=0.920, est=0.927, cfg={'max_features': 4, 'n_estimators': 100}\n",
      ">acc=0.950, est=0.927, cfg={'max_features': 4, 'n_estimators': 500}\n",
      ">acc=0.910, est=0.918, cfg={'max_features': 2, 'n_estimators': 100}\n",
      ">acc=0.930, est=0.924, cfg={'max_features': 6, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X[train_ix, :], X[test_ix, :]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    # configure the cross-validation procedure\n",
    "    cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "    # define the model\n",
    "    model = RandomForestClassifier(random_state=1)\n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['n_estimators'] = [10, 100, 500]\n",
    "    space['max_features'] = [2, 4, 6]\n",
    "    # define search\n",
    "    search = GridSearchCV(model, space, scoring='accuracy',\n",
    "                          cv=cv_inner, refit=True)\n",
    "    # execute search\n",
    "    result = search.fit(X_train, y_train)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    acc = accuracy_score(y_test, yhat)\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f, est=%.3f, cfg=%s' %\n",
    "          (acc, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated nested CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Automatic nested cross-validation for random forest on a classification dataset.\n",
    "- A simpler way that we can perform the same procedure is by using the cross_val_score() function that will execute the outer cross-validation procedure. \n",
    "- This can be performed on the configured GridSearchCV directly that will automatically use the refit best performing model on the test set from the outer loop. This greatly \n",
    "reduces the amount of code required to perform the nested cross-validation.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20,\n",
    "                           random_state=1, n_informative=10, n_redundant=10)\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "# define the model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "# define search space\n",
    "space = dict()\n",
    "space['n_estimators'] = [10, 100, 500]\n",
    "space['max_features'] = [2, 4, 6]\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy',\n",
    "                      n_jobs=1, cv=cv_inner, refit=True)\n",
    "# configure the cross-validation procedure\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# execute the nested cross-validation\n",
    "scores = cross_val_score(\n",
    "    search, X, y, scoring='accuracy', cv=cv_outer, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- We can sew how the estimated accuracies are different, but similar. \n",
    "- We can also see that different hyperparameters are found on each iteration, showing how hyperparameters can dependent on the specifics of the dataset.\n",
    "\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
